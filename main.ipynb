{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c3f894-52f4-4f17-a01c-e62b4bd5e50b",
   "metadata": {},
   "source": [
    "Sources of assessment: <br>\n",
    "https://colab.research.google.com/drive/1S2j8Hzwt3qJzc0sawizX_bsLdqiZM-0_#scrollTo=fz1JcLXD7d_k <br>\n",
    "https://gist.github.com/inoue-cw/ca171ed5e3e063e1fbaed265ec05b694 <br>\n",
    "https://ies.ed.gov/ncee/rel/regions/northeast/onlinetraining/ResourcesTools/Bloom%27s%20Taxonomy.pdf <br>\n",
    "\n",
    "Sources of OpenAI API:<br>\n",
    "https://platform.openai.com/docs/guides/text?lang=python\n",
    "\n",
    "Sources Bloom's <br>\n",
    "https://tophat.com/blog/blooms-taxonomy-question-stems/ <br>\n",
    "https://www.coloradocollege.edu/other/assessment/how-to-assess-learning/learning-outcomes/blooms-revised-taxonomy.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0eb526-705b-4355-aea6-16d6ef63f4cb",
   "metadata": {},
   "source": [
    "# Part A: LLM Analysis & Human Validation\n",
    "\n",
    "Write code that uses LLMs to perform initial analysis, then critically evaluate their outputs:\n",
    "\n",
    "1. **LLM-Generated Classifications**\n",
    "   - Write code to have the LLM classify each question's Bloom Level (1-6 scale)\n",
    "   - Code LLM assessment of question quality and potential issues\n",
    "   - Have LLM generate improved versions of questions (fix issues AND elevate to higher Bloom levels when possible, while keeping the same content)\n",
    "\t   - Increase Bloom level especially for questions that fall on the first level\n",
    "   - Get LLM recommendations for format conversions (MCQ → MCA/TEXT/NUMERIC)\n",
    "\n",
    "2. **Human Validation & Critical Analysis**\n",
    "   - **Agreement Check**: Do you agree with the LLM's Bloom classifications? Why or why not?\n",
    "   - **LLM Check Review**: Verify the validity of quality issues the LLM identifies across the question set, did it miss anything important?\n",
    "   - **Critical Gaps**: Where does the LLM seem confident but wrong? Where is it uncertain but correct?\n",
    "\n",
    "3. **Improvement & Format Evaluation**\n",
    "   - **Improvement Quality**: Are the LLM's suggested question improvements actually better?\n",
    "   - **Format Recommendations**: Do the LLM's format conversion suggestions make sense?\n",
    "\n",
    "\n",
    "4. **Synthesis & Insights**\n",
    "   - Document patterns in LLM performance (what it does well vs. poorly)\n",
    "   - Identify which types of analysis require human oversight\n",
    "   - Note questions where human judgment significantly differs from LLM assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26af848-cd61-4f7a-acf9-d1a4500e8b3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Importing modules and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d767514-1e80-4307-81ac-2689da52bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# carrega variáveis do .env para o ambiente do processo\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()  # Lê OPENAI_API_KEY do ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9324fe-3912-4b1e-bfd8-91ebf96be228",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FILE = 'C:/Users/Pedro Malavota/Desktop/CloudWalk/meu_projeto/Dataset.csv'\n",
    "\n",
    "df_questions = pd.read_csv(PATH_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b6ce7e-bdb2-4792-bddf-7abb8134caf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 23\n",
      "ID                                                               23\n",
      "Question          You're analyzing customer churn and find that ...\n",
      "Option_A                Users naturally become more loyal over time\n",
      "Option_B                 Early user experience may need improvement\n",
      "Option_C                               Seasonal effects in the data\n",
      "Option_D                                    Sample size differences\n",
      "Correct_Answer                                                    B\n",
      "Name: 22, dtype: object\n"
     ]
    }
   ],
   "source": [
    "i = 22\n",
    "print(f\"Question {i+1}\")\n",
    "#print(df_questions.iloc[i,1])\n",
    "print(df_questions.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a38e0f4d-d55b-41cb-b939-c460cba8c084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_answers = [3, 1, 1, 5, 1, 2, 2, 4, 4, 5, 3, 5, 1, 2, 1, 2, 1, 4, 1, 1, 5, 1, 5, 1, 1, 4, 2, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Load my answers\n",
    "\n",
    "my_answers1_10 = [3,1,1,5,1,2,2,4,4,5]\n",
    "my_answers11_20 = [3,5,1,2,1,2,1,4,1,1]\n",
    "my_answers21_30 = [5,1,5,1,1,4,2,1,1,2]\n",
    "my_answers = my_answers1_10+my_answers11_20+my_answers21_30\n",
    "print(f\"my_answers = {my_answers[0:30]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95be1d8b-18a6-4b3f-a240-15c1e4253000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"hello world!\"\n",
      "I'm ChatGPT.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "    say \"hello world!\" and state your name\n",
    "    \"\"\"\n",
    "response = client.responses.create(\n",
    "    model= \"gpt-5-2025-08-07\",\n",
    "    input= prompt\n",
    ");\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf995c6-2a52-40e3-a129-a1d1c2597fec",
   "metadata": {},
   "source": [
    "### Run models and save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "afa2306d-b443-44e4-9b13-7b0f53093411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(label, start, stop):\n",
    "    \"\"\"\n",
    "    Função para processar questões com diferentes modelos de LLM e salvar resultados em disco.\n",
    "    Args:\n",
    "        label (str): Rótulo do modelo a ser utilizado (ex: \"classify_bloom_level\").\n",
    "        start (int): Índice de início no DataFrame.\n",
    "        stop (int): Índice de parada no DataFrame.\n",
    "    Returns:\n",
    "        list: Uma lista com os resultados do processamento.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Define pasta de saída (mesmo nome da função/label)\n",
    "    output_dir = os.path.join(\"outputs\", label)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepara a lista de dados em um único lugar\n",
    "    for i in range(start, stop):\n",
    "        question = df_questions.iloc[i, 1]  # coluna \"Question\"\n",
    "        options = df_questions.iloc[i, 2:6].tolist()  # colunas Option_A até Option_D\n",
    "        correct_answer = df_questions.iloc[i, 6]  # coluna \"Correct_Answer\"\n",
    "        ID = df_questions.iloc[i, 0] # coluna \"ID\"\n",
    "\n",
    "        if label == \"classify_bloom_level\":\n",
    "            result = classify_bloom_level(question, options, correct_answer)\n",
    "        elif label == \"validate_question_quality\":\n",
    "            result = validate_question_quality(question, options, correct_answer)\n",
    "        elif label == \"suggest_question_improvements\":\n",
    "            result = suggest_question_improvements(question, options, correct_answer)\n",
    "        elif label == \"recommend_format_conversion\":\n",
    "            result = recommend_format_conversion(question, current_format=\"MCQ\")\n",
    "        elif label == \"create_format_conversion\":\n",
    "            new_format = input(f\"Choose question {ID} format (MCA, NUMERIC, TEXT): \\n\")\n",
    "            result = create_format_conversion(question, new_format, current_format=\"MCQ\")\n",
    "        else:\n",
    "            result = {\"error\": f\"COULD NOT READ QUESTION {i}\"}\n",
    "\n",
    "        # Salva resultado em disco (um arquivo por questão)\n",
    "        file_name = file_name = f\"{label}_q{ID}.json\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50739241-3edb-4f22-8182-d55e42234aab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Classify Bloom level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7aa70cf-9adc-48ab-ba06-42811c073688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_bloom_level(question, options, correct_answer):\n",
    "    \"\"\"Use LLM to classify Bloom taxonomy level (1-6) with reasoning\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following multiple-choice question and classify its Bloom's Taxonomy cognitive level (1-6).\n",
    "    \n",
    "    Question: {question}\n",
    "    Options: {options}\n",
    "    Correct Answer: {correct_answer}\n",
    "    \n",
    "    Bloom's Taxonomy Levels and Subcategories (in parenthesis):\n",
    "    1. Remember (Recognizing,Recalling)\n",
    "    2. Understand (Interpreting,Exemplifying,Classifying,Summarizing,Inferring,Comparing,Explaining)\n",
    "    3. Apply (Executing,Implementing)\n",
    "    4. Analyze (Differentiating,Organizing,Attributing)\n",
    "    5. Evaluate (Checking,Critiquing)\n",
    "    6. Create (Generating,Planning,Producing)\n",
    "    \n",
    "    Answer format (txt):\n",
    "    {{\n",
    "        \"level\": <int from 1 to 6>\n",
    "    }}\n",
    "    \"\"\"\n",
    "    # \"reasoning\": \"<concise reasoning (<40 words)>\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model= \"gpt-5-mini\",\n",
    "        reasoning= { \"effort\": \"medium\" },\n",
    "        instructions= \"You are an expert in educational assessment and Bloom’s Taxonomy classification.\",\n",
    "        input= prompt\n",
    "    );\n",
    "    \n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c4c7319-249c-464e-b2ef-9dd98aff114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 1, 2, 1, 1, 1, 3, 2, 2, 3, 1, 1, 1, 1, 2, 1, 2, 1, 1, 5, 1, 2, 1, 1, 3, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "data = run_models(\"classify_bloom_level\", 0, 30)\n",
    "\n",
    "output = []\n",
    "for item in data:\n",
    "    try:\n",
    "        item = json.loads(item)[\"level\"]\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        item = f\"Error: {e}\"   # or you could set issues = None\n",
    "    output.append(item)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea939f-b83a-44b3-b29e-fb2562e877a9",
   "metadata": {},
   "source": [
    "After batch-testing: <br>\n",
    "classifications_medium_reasoning gpt-5-mini = [3, 1, 1, 2, 1, 1, 1, 3, 2, 2, 3, 1, 1, 1, 1, 2, 1, 2, 1, 1, 5, 1, 2, 1, 1, 3, 1, 1, 1, 2] <br>\n",
    "classifications_low_reasoning gpt-5 = [3, 1, 1, 2, 1, 1, 1, 3, 1, 2, 3, 2, 1, 1, 1, 2, 1, 2, 1, 1, 4, 1, 4, 1, 1, 3, 2, 1, 1, 2] <br>\n",
    "classifications_medium_reasoning gpt-5 = [3, 1, 1, 3, 1, 1, 1, 3, 1, 2, 3, 2, 1, 1, 1, 2, 1, 1, 1, 1, 4, 1, 4, 1, 1, 3, 2, 1, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf265d7-3edf-4090-aa48-e9892c82e598",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Validate question quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5d5c1f4-bbe6-4521-9c74-adb4a75712b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_question_quality(question, options, correct_answer):\n",
    "\n",
    "    \"\"\"\n",
    "    Use LLM to check for quality issues in sample questions with reasoning.\n",
    "    Parameters:\n",
    "        question (str): enunciado da questão\n",
    "        options (list): alternativas possíveis\n",
    "        correct_answer (str): resposta correta\n",
    "    Returns:\n",
    "        dict: {\"issues\": str, \"reasoning\": str}\n",
    "    \"\"\"\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following multiple-choice question and check for relevant quality issues. If no issue is relevant, answer \"none\". \n",
    "    \n",
    "    Question: {question}\n",
    "    Options: {options}\n",
    "    Correct Answer: {correct_answer}\n",
    "\n",
    "    Examples of question quality issues: Ambiguity, poor phrasing, unecessary complexity, oversimplification.\n",
    "    Examples of options quality issues: More than one correct option, no correct option, wrong option mentioned as correct answer\n",
    "    \n",
    "    Answer format (txt):\n",
    "    {{ \n",
    "        \"issues\": \"<list issues (max 3 words)>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    # \"issues\": \"<list issues (max 3 words)>\"\n",
    "    # \"reasoning\": \"<concise reasoning (max 40 words)>\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "\treasoning= { \"effort\": \"low\" },\n",
    "\tinstructions= \"You are an expert in data analysis assessments for professional recruitment.\",\n",
    "\tinput= prompt\n",
    "    );\n",
    "    \n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "292fcaaf-a4c9-4cbc-bf0b-230729fde55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['none', 'none', 'none', 'ambiguous, multiple correct', 'none', 'wrong correct answer', 'imprecise wording', 'insufficient info, multiple correct options', 'none', 'none', 'none', 'oversimplification, metric ambiguity', 'none', 'none', 'none', 'none', 'none', 'term ambiguity, missing sample size, oversimplification', 'imprecision', 'none', 'Ambiguity, Multiple correct options', 'none', 'multiple correct answers, ambiguous', 'oversimplification', 'none', 'none', 'misleading options, incorrect distractors', 'Ambiguity, Multiple interpretations', 'none', 'none']\n"
     ]
    }
   ],
   "source": [
    "data_quality = run_models(\"validate_question_quality\", 0, 30)\n",
    "output = []\n",
    "for item in data_quality:\n",
    "    try:\n",
    "        issues = json.loads(item)[\"issues\"]\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        issues = f\"Error: {e}\"   # or you could set issues = None\n",
    "    output.append(issues)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ced08-39ff-4568-bad5-efb40bfd3a05",
   "metadata": {},
   "source": [
    "batch-testing gpt-5-mini <br>\n",
    "issues = ['none', 'none', 'none', 'ambiguous, multiple correct', 'none', 'wrong correct answer', 'imprecise wording', 'insufficient info, multiple correct options', 'none', 'none', 'none', 'oversimplification, metric ambiguity', 'none', 'none', 'none', 'none', 'none', 'term ambiguity, missing sample size, oversimplification', 'imprecision', 'none', 'Ambiguity, Multiple correct options', 'none', 'multiple correct answers, ambiguous', 'oversimplification', 'none', 'none', 'misleading options, incorrect distractors', 'Ambiguity, Multiple interpretations', 'none', 'none'] <br>\n",
    "batch-testing gpt-5: <br>\n",
    "issues = ['oversimplification', 'none', 'none', 'Nonparallel options', 'Ambiguity, oversimplification', 'Incorrect answer key, Ambiguous wording', 'oversimplification', 'Multiple correct options, Missing sample size, Ambiguous phrasing', 'none', 'none', 'oversimplification', 'none', 'none', 'none', 'none', 'none', 'none', 'Terminology ambiguity', 'oversimplification, minor ambiguity', 'none', 'none', 'none', 'Ambiguity, Multiple correct options, Oversimplification', 'oversimplification', 'none', 'none', 'none', 'Ambiguity, No correct option', 'none', 'none']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c2c2a-f60e-43fb-8abf-8938a8fe2c6b",
   "metadata": {},
   "source": [
    "### Suggest question improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b905a57-8340-4235-9a00-ea7368b578b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_question_improvements(question, options, correct_answer):\n",
    "\n",
    "    \"\"\"\n",
    "    Use LLM to check for quality issues in sample questions with reasoning.\n",
    "    Parameters:\n",
    "        question (str): enunciado da questão\n",
    "        options (list): alternativas possíveis\n",
    "        correct_answer (str): resposta correta\n",
    "    Returns:\n",
    "        dict: {\"issues\": str, \"reasoning\": str}\n",
    "    \"\"\"\n",
    "\n",
    "    level = classify_bloom_level(question, options, correct_answer)\n",
    "    issues = validate_question_quality(question, options, correct_answer)\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "    Generate an improved version of the question, with one higher level on the Bloom Scale and no remaining issues, do not change the content of the question.\n",
    "    Consider the inputs of taxonomy level and detected issues. If the question is level 1, always push it to level 2 or 3.\n",
    "\n",
    "    Good example: <original question>Which pandas function reads CSV files?<\\original question> <final question>Explain the purpose of the pandas function \\'read_csv()\\' in data analysis.<\\final question> The new question requires students to not only recall the function name but also to understand its purpose and application in data analysis, which aligns with the \\'Understand\\' level of Bloom\\'s Taxonomy.\n",
    "    Bad example: <original question>What does the correlation coefficient measure?<\\original question> <final question>How would you explain the significance of the correlation coefficient in statistical analysis?<\\final question> The new question only changed the wording, but the thinking process remained the same.\n",
    "   \n",
    "\n",
    "    Question: {question}\n",
    "    Options: {options}\n",
    "    Correct Answer: {correct_answer}\n",
    "    \n",
    "    Bloom's Taxonomy Levels and Subcategories (in parenthesis):\n",
    "    1. Remember (Recognizing,Recalling)\n",
    "    2. Understand (Interpreting,Exemplifying,Classifying,Summarizing,Inferring,Comparing,Explaining)\n",
    "    3. Apply (Executing,Implementing)\n",
    "    4. Analyze (Differentiating,Organizing,Attributing)\n",
    "    5. Evaluate (Checking,Critiquing)\n",
    "    6. Create (Generating,Planning,Producing)\n",
    "    \n",
    "    Answer format (txt):\n",
    "    {{\n",
    "        \"predicted level\": <int from 1 to 6>,\n",
    "        \"new question\": \"<question generated by LLM>\",\n",
    "        \"new level\": <int from 1 to 6>,\n",
    "    \t\"new options\": \"<options generated by LLM>\",\n",
    "    \t\"new correct option\": \"<letter between a,b,c,d>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # \"reasoning\": \"<concise reasoning (<40 words)>\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "\treasoning= { \"effort\": \"medium\" },\n",
    "\tinstructions= \"You are an expert in data analysis assessments for professional recruitment.\",\n",
    "\tinput= prompt\n",
    "    );\n",
    "    \n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b31c2a9-ecde-479d-afd9-107c2a58502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_improved = run_models(\"suggest_question_improvements\", 0,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4f45e33-e5d6-487a-8204-3e7267e3efc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'questoes_melhoradas.csv' salvo com sucesso com 30 questões!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# Supondo que a variável 'questions_improved' já existe e foi preenchida pela sua função\n",
    "# Exemplo de como 'questions_improved' poderia ser:\n",
    "# questions_improved = [\n",
    "#     '{\"predicted level\": 2, \"new question\": \"Qual a finalidade da função `read_csv` do pandas?\", \"new options\": \"a) Ler arquivos Excel\\\\nb) Ler arquivos de texto\\\\nc) Ler arquivos CSV para um DataFrame\\\\nd) Escrever em arquivos CSV\", \"new correct option\": \"c\"}',\n",
    "#     '{\"predicted level\": 3, \"new question\": \"Como você aplicaria a função `groupby` para calcular a média de salário por departamento?\", \"new options\": \"a) df.groupby(\\'departamento\\').mean()\\\\nb) df.mean().groupby(\\'departamento\\')\\\\nc) df.groupby(\\'departamento\\')[\\'salario\\'].mean()\\\\nd) df[\\'salario\\'].groupby().mean()\", \"new correct option\": \"c\"}'\n",
    "# ]\n",
    "\n",
    "\n",
    "# 1. Preparar os dados para o CSV\n",
    "dados_para_csv = []\n",
    "for resultado_str in questions_improved:\n",
    "    try:\n",
    "        # Converte a string JSON em um dicionário Python\n",
    "        dados_dict = json.loads(resultado_str)\n",
    "        \n",
    "        # Extrai os dados desejados. Usar .get() é mais seguro caso a chave não exista.\n",
    "        nova_questao = dados_dict.get(\"new question\", \"N/A\")\n",
    "        novas_opcoes = dados_dict.get(\"new options\", \"N/A\")\n",
    "        \n",
    "        # Adiciona um dicionário com os dados extraídos à nossa lista\n",
    "        dados_para_csv.append({\n",
    "            'nova_questao': nova_questao,\n",
    "            'novas_opcoes': novas_opcoes\n",
    "        })\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Aviso: Não foi possível processar o seguinte resultado por não ser um JSON válido:\\n{resultado_str}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro inesperado: {e}\")\n",
    "\n",
    "\n",
    "# 2. Escrever os dados em um arquivo CSV\n",
    "nome_arquivo_csv = 'questoes_melhoradas.csv'\n",
    "colunas = ['nova_questao', 'novas_opcoes']\n",
    "\n",
    "try:\n",
    "    with open(nome_arquivo_csv, 'w', newline='', encoding='utf-8') as arquivo_csv:\n",
    "        # Cria um \"escritor\" de CSV que trabalha com dicionários\n",
    "        writer = csv.DictWriter(arquivo_csv, fieldnames=colunas)\n",
    "        \n",
    "        # Escreve a linha de cabeçalho (header)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Escreve todas as linhas de dados de uma vez\n",
    "        writer.writerows(dados_para_csv)\n",
    "        \n",
    "    print(f\"Arquivo '{nome_arquivo_csv}' salvo com sucesso com {len(dados_para_csv)} questões!\")\n",
    "\n",
    "except IOError:\n",
    "    print(f\"Erro: Não foi possível escrever no arquivo '{nome_arquivo_csv}'. Verifique as permissões.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b606f-25d4-4c60-9209-a1a25d4aaa62",
   "metadata": {},
   "source": [
    "Batch testing gpt-5-mini: (new questions) <br>\n",
    "['Explain what the mean (average) of the dataset [2, 4, 6, 8] represents and calculate its value.', \"Explain the purpose of the pandas function 'read_csv()' in a data analysis workflow.\", \"Explain why approximately 68% of values in a normal distribution fall within one standard deviation of the mean, and what this indicates about the distribution's variability.\", 'For the dataset [1, 1, 2, 3, 100], calculate the mean and the median, then identify which measure of central tendency best represents the typical values and explain your reasoning.', 'Explain what the correlation coefficient measures with respect to both the direction and strength of a linear relationship between two variables, and how its numerical values are interpreted.', 'Which Python library is primarily used for data manipulation, offering DataFrame structures and high-level data operations?', 'Explain what a p-value represents in hypothesis testing and how it is used when deciding whether to reject the null hypothesis.', 'Explain which statistical test is most appropriate to determine whether the observed difference between two website conversion rates (2.1% and 2.8%) is statistically significant, and briefly justify your choice.', 'In a right-skewed distribution caused by a few large outliers, which statement best describes the typical relationship between the mean and the median?', 'Explain in which situation you would choose a scatter plot instead of a bar chart.', 'Explain the correct method for finding the median of the dataset [1, 3, 5, 7, 9], then identify the median.', 'Interpret the model performance: it achieves 85% accuracy on the training set but only 65% on the test set. Which of the following issues best explains this discrepancy?', 'Which SQL clause is used to filter rows? Choose the option that correctly identifies the clause and explains its purpose in a query.', 'In an A/B test comparing a new website design to the existing one, which group is the control group and why is that group used?', 'Which pandas method removes duplicate rows and (by default) returns a DataFrame with those duplicates removed?', 'Given that your sales data shows a spike every December, identify which type of time-series pattern this represents and briefly explain your reasoning.', \"Explain the purpose of the matplotlib function 'hist()' and describe a situation in data visualization where it is the appropriate choice.\", 'Explain which statistical inputs you must know to determine whether an observed 95% customer satisfaction rate is statistically significantly different from the industry average of 90%.', 'Explain the primary purpose of the SQL GROUP BY clause and, from the options below, choose the best description of what it accomplishes.', 'Explain which of the following measures best describes the spread of a dataset and justify your choice.', 'When cleaning data, you find 15% missing values in a key variable. What is the first action you would take in your data-cleaning workflow to address this issue?', \"Describe what the box in a box plot represents and explain how this summary helps in understanding a dataset's distribution.\", 'Design a specific analysis or experiment to test whether the higher churn in months 1–3 (40%) versus months 4–6 (15%) is due to a poorer early user experience. Which of the following would be the most direct and effective first step?', \"Explain the purpose of NumPy's std() function and describe a situation in data analysis where computing the standard deviation would be useful.\", 'Describe the information returned by df.shape in pandas and explain when you would use it during data inspection.', 'You want to compare average sales across 5 different regions. Which statistical test is most appropriate, and briefly explain why this test is preferable to the other options listed?', 'Which statement best explains the difference between pandas .loc and .iloc when selecting data from a DataFrame?', 'Explain what the whiskers in a box plot typically represent in a dataset and how they relate to the quartiles and outliers.', \"Explain what is meant by 'rejecting a true null hypothesis' and select the term that correctly names this error.\", 'Identify which type of analytics is being used when a marketing team predicts customer lifetime value using past purchase behavior, demographics, and engagement metrics, and briefly explain why this classification is appropriate.']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce3e75-de53-4ec0-bce4-3575de5263e1",
   "metadata": {},
   "source": [
    "### Recommend Format Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "02ee9945-371d-44a7-88a1-9ff1d99e6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_format_conversion(question, current_format=\"MCQ\"):\n",
    "    prompt = f\"\"\"\n",
    "Analyze the multiple-choice question and assign new format to question.\n",
    "\n",
    "Good examples of changing format:\n",
    "- Question too specific or too simple → MCA \n",
    "- Calculation where options may add bias → NUMERIC\n",
    "- Question could assess higher levels of cognitive complexity (Checking, Critiquing, Generating, Planning, Producing)→ TEXT\n",
    "\n",
    "Question: {question}\n",
    "Current format: {current_format}\n",
    "\n",
    "Formats:\n",
    "MCQ: One correct option\n",
    "MCA: Multiple correct options\n",
    "TEXT: Short free response (max 3 sentences)\n",
    "NUMERIC: Numeric answer\n",
    "\n",
    "Answer format (txt):\n",
    "{{\n",
    "    \"new format\": \"<choose \"MCA\", \"TEXT\" or \"NUMERIC\">\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        instructions=\"You are an expert in recruitment assessments.\",\n",
    "        input=prompt\n",
    "    )\n",
    "    \n",
    "    return response.output_text\n",
    "\n",
    "\n",
    "# said to only change format to produce meaningful impact\n",
    "# put less unecessary words (verbs and synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d2dc5f8b-0327-4dee-9ebc-ad34f2da75e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\\n    \"new format\": \"NUMERIC\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"TEXT\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"TEXT\"\\n}', '{\\n    \"new format\": \"TEXT\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"TEXT\"\\n}', '{\\n    \"new format\": \"NUMERIC\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"TEXT\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"TEXT\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"TEXT\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"TEXT\"\\n}', '{\\n    \"new format\": \"TEXT\"\\n}', '{\\n    \"new format\": \"TEXT\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}', '{\\n    \"new format\": \"MCA\"\\n}']\n"
     ]
    }
   ],
   "source": [
    "new_format = run_models(\"recommend_format_conversion\", 0,30)\n",
    "print(new_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "37feef11-c336-4dae-9ce4-86067ce4e4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NUMERIC', 'MCA', 'MCA', 'MCA', 'TEXT', 'MCA', 'TEXT', 'TEXT', 'MCA', 'TEXT', 'NUMERIC', 'MCA', 'MCA', 'MCA', 'MCA', 'MCA', 'MCA', 'MCA', 'TEXT', 'MCA', 'TEXT', 'MCA', 'TEXT', 'MCA', 'TEXT', 'TEXT', 'TEXT', 'MCA', 'MCA', 'MCA']\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for item in new_format:\n",
    "    try:\n",
    "        format = json.loads(item)[\"new format\"]\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        format = f\"Error: {e}\"   # or you could set issues = None\n",
    "    output.append(format)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881784e-6481-4390-a741-28961bc90ae0",
   "metadata": {},
   "source": [
    "After batch-testing: <br>\n",
    "['NUMERIC', 'MCA', 'MCA', 'MCA', 'TEXT', 'MCA', 'TEXT', 'TEXT', 'MCA', 'TEXT', 'NUMERIC', 'MCA', 'MCA', 'MCA', 'MCA', 'MCA', 'MCA', 'MCA', 'TEXT', 'MCA', 'TEXT', 'MCA', 'TEXT', 'MCA', 'TEXT', 'TEXT', 'TEXT', 'MCA', 'MCA', 'MCA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7906d7a-5a22-4b1c-ad08-1950b518aff4",
   "metadata": {},
   "source": [
    "### Create format conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bdf09605-7558-4457-b35a-771e0a344768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_format_conversion(question, new_format, current_format=\"MCQ\"):\n",
    "    \"\"\"Convert a question into another type\"\"\"\n",
    "\n",
    "    promptMCA = f\"\"\"\n",
    "Analyze the question and generate a new question with format of multiple correct options. \n",
    "Do not change the question topic. Generate 4 different options labeled from \"A\" to \"D\".\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "Current format: {current_format}\n",
    "\n",
    "Answer format (txt):\n",
    "{{\n",
    "    \"new_question\": \"<new question>\",\n",
    "    \"new_options\": \"<new options\">,\n",
    "    \"new_correct_answer\": <label of correct new alternatives>\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    promptTYPING = f\"\"\"\n",
    "Analyze the question and generate new question with new format.\n",
    "\n",
    "Question: {question}\n",
    "Current format: {current_format}\n",
    "New format: {new_format}\n",
    "\n",
    "Formats:\n",
    "TEXT: Short free response (max 3 sentences)\n",
    "NUMERIC: Numeric answer\n",
    "\n",
    "Answer format (txt):\n",
    "{{\n",
    "    \"new_question\": \"<TEXT, NUMERIC>\",\n",
    "    \"correct_answer\": <int, string>\n",
    "}}\n",
    "\"\"\"\n",
    "    if (new_format == \"MCA\"):\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5-mini\",\n",
    "            reasoning={\"effort\": \"medium\"},\n",
    "            instructions=\"You are an expert in recruitment assessments.\",\n",
    "            input=promptMCA\n",
    "    )\n",
    "\n",
    "    else:\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5-mini\",\n",
    "            reasoning={\"effort\": \"low\"},\n",
    "            instructions=\"You are an expert in recruitment assessments.\",\n",
    "            input=promptTYPING\n",
    "    )\n",
    "        \n",
    "    \n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "00402799-f160-4082-bdce-22968ca01cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose question 1 format (MCA, NUMERIC, TEXT): \n",
      " NUMERIC\n",
      "Choose question 2 format (MCA, NUMERIC, TEXT): \n",
      " MCA\n",
      "Choose question 3 format (MCA, NUMERIC, TEXT): \n",
      " MCA\n",
      "Choose question 4 format (MCA, NUMERIC, TEXT): \n",
      " MCA\n",
      "Choose question 5 format (MCA, NUMERIC, TEXT): \n",
      " TEXT\n"
     ]
    }
   ],
   "source": [
    "converted_formats = run_models(\"create_format_conversion\", 0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317dd5ad-9805-479e-ad1d-685b844b01c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Part B: Format Exploration & Open Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8ea9d7d9-a825-41e4-85ac-8b6a14d4c142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'When would you use a scatter plot instead of a bar chart?', 'answer': 'Relationship analysis.', 'verdict': 'Right'}\n"
     ]
    }
   ],
   "source": [
    "def short_answer(question: str) -> str:\n",
    "    \"\"\"LLM-1: Generates a concise 2-3 word answer.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # can use gpt-4o or gpt-3.5 depending on your plan\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer each question in 2-3 words only.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def judge_answer(question: str, answer: str) -> str:\n",
    "    \"\"\"LLM-2: Judges if the given answer is correct or wrong.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a strict grader. Reply only with 'Right' or 'Wrong'.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\\nAnswer: {answer}\\nIs the answer correct?\"}\n",
    "        ],\n",
    "        max_tokens=5\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def conflict_mode(question: str):\n",
    "    \"\"\"Runs the two-LLM conflict process.\"\"\"\n",
    "    ans = short_answer(question)\n",
    "    verdict = judge_answer(question, ans)\n",
    "    return {\"question\": question, \"answer\": ans, \"verdict\": verdict}\n",
    "\n",
    "# Example usage\n",
    "question = \"When would you use a scatter plot instead of a bar chart?\"\n",
    "result = conflict_mode(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e29621-2ae8-4536-8e09-3f258c1a23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arg(answer):\n",
    "    \"\"\"Break answer into main arguments\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def arg_logic(arg):\n",
    "    \"\"\"Decide whether argument makes logical sense\n",
    "    if valid, inconclusive, invalid -> score 1, 0.5, 0\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def arg_context(arg,question):\n",
    "    \"\"\"Decide whether argument is conected to context\n",
    "    if directly conected, indirectly conected, disconected -> score 1, 0.5,0 \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def score_question(answer, question):\n",
    "    arguments = tokenize_answer(answer)\n",
    "    score = 0\n",
    "    len(arguments) = N\n",
    "    for arg in range(N):\n",
    "        logic_score = arg_logic(arg)\n",
    "        context_score = arg_context\n",
    "        score += 1/N * logic_score * context_score\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dbcf15d-a05c-43b5-8e86-e3fd5fc4ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# ------------------------\n",
    "# Funções de suporte\n",
    "# ------------------------\n",
    "\n",
    "def get_arg(answer: str) -> list:\n",
    "    \"\"\"Break answer into main arguments using GPT\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Break the following answer into a list of its main arguments.\n",
    "    Return ONLY a JSON object with a key 'arguments' that contains a list of strings.\n",
    "\n",
    "    Answer: {answer}\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    args = response.choices[0].message.content\n",
    "    data = json.loads(args)\n",
    "    return data[\"arguments\"]\n",
    "\n",
    "\n",
    "def arg_logic(arg: str) -> float:\n",
    "    \"\"\"Evaluate logical validity of argument\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Evaluate the logical validity of this argument: \"{arg}\"\n",
    "\n",
    "    Categories:\n",
    "    - valid -> score 1\n",
    "    - mostly valid -> score 2/3\n",
    "    - invalid/contradictory -> score 0\n",
    "\n",
    "    Answer ONLY with a number (1, 0.5, or 0).\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    return float(score_text)\n",
    "\n",
    "\n",
    "def arg_context(arg: str, question: str) -> float:\n",
    "    \"\"\"Evaluate contextual relevance of argument to question\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Evaluate whether the argument is connected to the question.\n",
    "\n",
    "    Question: {question}\n",
    "    Argument: {arg}\n",
    "\n",
    "    Categories:\n",
    "    - directly connected -> score 1\n",
    "    - indirectly connected -> score 0.5\n",
    "    - disconnected -> score 0\n",
    "\n",
    "    Answer ONLY with a number (1, 0.5, or 0).\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    return float(score_text)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Função principal\n",
    "# ------------------------\n",
    "\n",
    "def score_question(answer: str, question: str) -> float:\n",
    "    \"\"\"Score an answer (0-10) based on logic + context of its arguments\"\"\"\n",
    "    arguments = get_arg(answer)\n",
    "    N = len(arguments)\n",
    "\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "\n",
    "    total_score = 0\n",
    "    invalid_count = 0\n",
    "\n",
    "    for arg in arguments:\n",
    "        logic_score = arg_logic(arg)\n",
    "        context_score = arg_context(arg, question)\n",
    "        arg_score = logic_score * context_score\n",
    "\n",
    "        total_score += arg_score\n",
    "\n",
    "    avg_score = total_score / N\n",
    "\n",
    "    final_score = 10 * avg_score\n",
    "    return max(0, round(final_score, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b56b3558-eff1-425d-975e-12115726572b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"You're analyzing customer churn and find that Month 1-3 users have 40% churn, Month 4-6 users have 15% churn. What insight should you investigate?\"\n",
    "a = \"The main insight is that older customers churn more than new ones. Therefore, you should focus only on Month 4–6 users.\"\n",
    "score_question(a,q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c6a31-244c-4f84-a4f0-8db860131174",
   "metadata": {},
   "source": [
    "Question = \"When would you use a scatter plot instead of a bar chart?\" <br>\n",
    "Answer 1 (good): You would use a scatter plot when you want to see the relationship between two continuous variables. For example, plotting income against education level reveals correlation patterns that a bar chart would hide. grade: 10.0 <br>\n",
    "Answer 2 (good): Scatter plots are useful when the goal is to detect clusters, trends, or outliers in paired data. Unlike bar charts, they show how one variable changes in relation to another. grade: 7.5 <br>\n",
    "Answer 3 (bad): A scatter plot is used when you want to count categories. It’s basically the same as a bar chart but with dots instead of bars. grade: 1.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cc69c-eb41-4967-913d-36cc7e85f36f",
   "metadata": {},
   "source": [
    "Question = “You're analyzing customer churn and find that Month 1-3 users have 40% churn, Month 4-6 users have 15% churn. What insight should you investigate?” <br>\n",
    "Answer 1 (good): The high churn in the first three months suggests problems with onboarding or early product adoption. It would be important to investigate user experience and engagement in the first weeks. grade: 7.5 <br>\n",
    "Answer 2 (good):This pattern implies that newer customers are not finding immediate value. You should investigate why early users drop off and what differentiates them from those who stay longer. grade: 6.5 <br>\n",
    "Answer 3 (bad): The main insight is that older customers churn more than new ones. Therefore, you should focus only on Month 4–6 users.The main insight is that older customers churn more than new ones. Therefore, you should focus only on Month 4–6 users. grade: 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cw_env)",
   "language": "python",
   "name": "cw_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
